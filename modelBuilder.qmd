---
title: "Model Builder"
author: "James"
format:
  html: 
    theme: "pulse"
    self-contained: true
    embed-resources: true
execute: 
  error: true
  warning: false
  eval: true
fig-asp: 0.618
fig-width: 10
---

## Imports

```{python imports}
import pandas as pd
import numpy as np
import importlib
from joblib import dump, load
from datetime import datetime as dt
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.models import Model
```

## Data Loading

I will use the FOMC speeches and compare how they affect the S&P500 Index.

```{python gather-data}
speechesDF = pd.read_csv("data/speechData.csv").iloc[:,1:] #Removes extra col

speechesDF.columns
speeches = speechesDF.groupby(['Title', 'Name', 'Date', 'Length'])['Text'].apply(list).reset_index()
snpDF = pd.read_csv("data/stocks/snp.csv").iloc[:,1:]
```

## Data Manipulation

I want to see the lasting effects from a speech. I will use the 5 day percent change for this. How much did the index change over the five days *after* the speech?

```{python data-manip}
snpDF["5day%Change"] = [(snpDF["snp_Close"][i + 5] - snpDF["snp_Close"][i]) / snpDF["snp_Close"][i] for i in range(len(snpDF["snp_Close"]) - 5)] + [0 for _ in range(5)]
```


Now, let's make sure we only consider the days that have both a speech and stock data.

```{python final-checks}
snp_using = snpDF[snpDF["Date"].isin(speechesDF["Date"])]
fomc_using = speechesDF[speechesDF["Date"].isin(snpDF["Date"])].drop_duplicates(subset=['Date']) #Removing multiple speeches on the same day for now
assert(len(fomc_using) == len(snp_using))

speeches = fomc_using["Text"][:-5]
labels = snp_using["5day%Change"][:-5] #Last 5 are 0's
```

# Individual Model Creating

## Preprocessing

```{python preprocessing}
# Parameters
max_vocab_size = 5000 # Limit on most common unique words
max_sequence_length = 100 # Limit on sentence length
embedding_dim = 128 # Word vector dimension

# Tokenization
tokenizer = Tokenizer(num_words=max_vocab_size)
tokenizer.fit_on_texts(speeches) # Creates a token for each word
sequences = tokenizer.texts_to_sequences(speeches) 
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post') #Push sequences to a fixed length

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size = 0.3, random_state = 10)
```

## Model Creation

```{python model-creation}
#| eval: false
def lstmModel():
    '''
    Method to create a Deep Learning Recurrent Neural Network
    '''
    model = Sequential([
        Embedding(input_dim=max_vocab_size, output_dim=embedding_dim),
        LSTM(128, return_sequences=True),
        Dropout(0.5), #prevent overfitting
        LSTM(64),
        Dropout(0.5),
        Dense(1)  # Regression output
    ])
    return model

# Creating two models: One with MSE and one with MAE as the error term
MSElstm = lstmModel()
MAElstm = lstmModel()

MSElstm.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])
MAElstm.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_error'])

MSElstm.fit(X_train, y_train, epochs=10, batch_size=32)
MAElstm.fit(X_train, y_train, epochs=10, batch_size=32)

dump(MSElstm, "models/speeches/MSElstm.joblib")
dump(MAElstm, "models/speeches/MAElstm.joblib")
```

```{python model-loading}
MSElstm = load("models/speeches/MSElstm.joblib")
MAElstm = load("models/speeches/MAElstm.joblib")
```

## Analysis

```{python testing-correctness}
mse = [mean_squared_error(MSElstm.predict(X_test), y_test), mean_squared_error(MAElstm.predict(X_test), y_test)]
mae = [mean_absolute_error(MSElstm.predict(X_test), y_test), mean_absolute_error(MAElstm.predict(X_test), y_test)]

# Comparing the models against each other
pd.DataFrame({
  "Model" : ["MSE", "MAE"],
  "MSE" : mse,
  "MAE" : mae
})
```

At a glance, this appears to handle the data quite well! Let's consider the plot of the actual change versus the predicted change.

## Visualization

```{python visual-imports}
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
import matplotlib.patches as mpatches
import seaborn as sns
```

```{python scatterplot}
df = pd.DataFrame({
  "Actual" : list(y_test),
  "MAE Predict" : [i[0] for i in list(MAElstm.predict(X_test))],
  "MSE Predict" : [i[0] for i in list(MSElstm.predict(X_test))]
}).sort_values(by='Actual').reset_index().drop(columns=["index"])

colors = sns.color_palette("colorblind")

# Plotting
plt.figure(figsize=(10, 6))
for i, column in enumerate(df.columns):
    sns.scatterplot(x=df.index, y=df[column], label=column, color=colors[i])

plt.title('5 Day Percent Change after FOMC Speech')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend(title='Datapoints')
plt.grid(True)
plt.show()
```

From the image, we can clearly see that the speeches aren't being fit well. We can actually go a step further as well. Lets take each of the points and assign it a value. It'll be green if it lies within a 0.01 threshold of the actual change and red otherwise. Here is the Mean Absolute Error model:

```{python individual-comparison-colored-mae}
df = pd.DataFrame({
    "Actual": list(y_test),
    "MAE Predict": [i[0] for i in list(MAElstm.predict(X_test))]
}).sort_values(by='Actual').reset_index().drop(columns=["index"])
df['Correctness'] = abs(df["Actual"] - df["MAE Predict"]) < 0.01

plt.clf()

df = df.reset_index()
fig, ax = plt.subplots(figsize=(14, 6))

# Define the color palette and map colors based on 'Correctness'
palette = sns.color_palette("colorblind", 2)
df['Color'] = df['Correctness'].map({True: palette[1], False: palette[0]})

# Actual scatterplot
ax.plot(df.index, df['Actual'], 'ko', label='Actual', markersize=5)  # 'ko' is dark color and circle markets

# Predict scatterplot
scatter = plt.scatter(x=df['index'], y=df['MAE Predict'], c=df['Color'], s=50, alpha=0.9, edgecolor='k')

# Manaually create legend to fix matplotlib issue
correct_patch = mpatches.Patch(color=palette[1], label='Correct')
incorrect_patch = mpatches.Patch(color=palette[0], label='Incorrect')

# Plot legend
plt.legend(handles=[correct_patch, incorrect_patch], loc='upper right')

# Axis labels
plt.grid(True)
plt.xlabel('Index')
plt.ylabel('Value')
plt.title('Actual vs MAE Predict with Correctness Color')

plt.tight_layout()
plt.show()
```

We can do the same process to see the results for the mean squared error model.

```{python individual-comparison-colored-mse}
df2 = pd.DataFrame({
    "Actual": list(y_test),
    "MSE Predict": [i[0] for i in list(MSElstm.predict(X_test))]
}).sort_values(by='Actual').reset_index().drop(columns=["index"])
df2['Correctness'] = abs(df["Actual"] - df2["MSE Predict"]) < 0.01

plt.clf()

df2 = df2.reset_index()
fig, ax = plt.subplots(figsize=(14, 6))

# Define the color palette and map colors based on 'Correctness'
palette = sns.color_palette("colorblind", 2)
df2['Color'] = df2['Correctness'].map({True: palette[1], False: palette[0]})

# Actual scatterplot
ax.plot(df2.index, df2['Actual'], 'ko', label='Actual', markersize=5)  # 'ko' is dark color and circle markets

# Predict scatterplot
scatter = plt.scatter(x=df2['index'], y=df2['MSE Predict'], c=df2['Color'], s=50, alpha=0.9, edgecolor='k')

# Manaually create legend to fix matplotlib issue
correct_patch = mpatches.Patch(color=palette[1], label='Correct')
incorrect_patch = mpatches.Patch(color=palette[0], label='Incorrect')

# Plot legend
plt.legend(handles=[correct_patch, incorrect_patch], loc='upper right')

# Axis labels
plt.grid(True)
plt.xlabel('Index')
plt.ylabel('Value')
plt.title('Actual vs MSE Predict with Correctness Color')

plt.tight_layout()
plt.show()
```


```{python correctness-overall}
print(f'MAE got {len([i for i in range(len(df)) if list(df["Correctness"])[i]]) * 100 / len(df)}% correct while MSE got {np.round(len([i for i in range(len(df)) if list(df2["Correctness"])[i]])* 100 / len(df), 2)}% correct')
```

We can see that, overall, the mean absolute error did better, but only fractionally. It was within a percent of the actual 5 day movement only 43.75% of the time. In order to raise this, we need more data to build from.

# Personal Neural Network Approach

I want to use my own Neural Network to see if it can compete with other, more developed models. We can use the same preprocessing setup.


```{python myNetwork-preprocessing}
# Parameters
max_vocab_size = 5000 # Limit on most common unique words
max_sequence_length = 100 # Limit on sentence length
embedding_dim = 128 # Word vector dimension

# Tokenization
tokenizer = Tokenizer(num_words=max_vocab_size)
tokenizer.fit_on_texts(speeches) # Creates a token for each word
sequences = tokenizer.texts_to_sequences(speeches) 
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post') #Push sequences to a fixed length

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size = 0.3, random_state = 10)

# Important to make input vectors into 
trainData = [(np.array(x).reshape(-1, 1), np.array(y).reshape(-1, 1)) for x,y in zip(X_train, y_train)]
testData = [(np.array(x).reshape(-1, 1), np.array(y).reshape(-1, 1)) for x,y in zip(X_test, y_test)]
```


```{python myNetwork-Implementation}
from scripts.myNeuralNet import Network
import numpy as np

myNetwork = Network([100, 50, 20, 1]) # Same setup as the popular model
myNetwork.SGD(training_data = trainData, epochs = 300, mini_batch_size = 32, learningRate = 0.1, test_data = testData)
```

Now, we can compare our personal network to that of the industry standard.
```{python individual-comparison-colored-mse}
df3 = pd.DataFrame({
    "Actual": [pair[1] for pair in testData],
    "Predicted": [myNetwork.feedForward(pair[0]) for pair in testData]
}).sort_values(by='Actual').reset_index().drop(columns=["index"])

df3['Correctness'] = abs(df3["Actual"] - df3["Predicted"]) < 0.01

plt.clf()

df3 = df3.reset_index()
fig, ax = plt.subplots(figsize=(14, 6))

# Define the color palette and map colors based on 'Correctness'
palette = sns.color_palette("colorblind", 2)
df3['Color'] = df3['Correctness'].map({True: palette[1], False: palette[0]})

# Actual scatterplot
ax.plot(df3.index, df3['Actual'], 'ko', label='Actual', markersize=5)  # 'ko' is dark color and circle markets

# Predict scatterplot
scatter = plt.scatter(x=df3['index'], y=df3['Predicted'], c=df3['Color'], s=50, alpha=0.9, edgecolor='k')

# Manaually create legend to fix matplotlib issue
correct_patch = mpatches.Patch(color=palette[1], label='Correct')
incorrect_patch = mpatches.Patch(color=palette[0], label='Incorrect')

# Plot legend
plt.legend(handles=[correct_patch, incorrect_patch], loc='upper right')

# Axis labels
plt.grid(True)
plt.xlabel('Index')
plt.ylabel('Value')
plt.title('Actual vs My Prediction with Correctness Color')

plt.tight_layout()
plt.show()
```

Or, if we want to compare it directly, we can just look at the correctness values. 

```{python correctness-overall}
print(f'MAE got {len([i for i in range(len(df)) if list(df["Correctness"])[i]]) * 100 / len(df)}% correct while MSE got {np.round(len([i for i in range(len(df2)) if list(df2["Correctness"])[i]])* 100 / len(df2), 2)}% correct. In comparison, my network got {len([i for i in range(len(df3)) if list(df3["Correctness"])[i]]) * 100 / len(df3)}% correct')
```

# Analysis

Overall, my network did just as well as the transformers network when using mean average error on these speeches. Although both of these performances were relatively lackluster (only had 45\% success with a 1\% threshold), it's cool to show how my neural network compares to those from the Transformers Import.

